# -*- coding: utf-8 -*-
import torch
import cv2
import numpy as np
import argparse
import os
import math # ç¡®ä¿ math è¢«å¯¼å…¥
import csv
from pathlib import Path
from tqdm import tqdm
from collections import deque

# å¯¼å…¥ä½ é¡¹ç›®é‡Œçš„æ„å»ºå™¨å’Œæ¨¡å‹ï¼
from models_factory.builder import build_model
from datasets_factory.transforms.utracknetv1_transforms import (
    Resize, ConcatChannels
)

# --- 1. â€œå¨æˆ¿é‡åœ°â€: è¾…åŠ©å‡½æ•°å’Œé…ç½® (è¿™éƒ¨åˆ†ä¸å˜) ---
model_cfg = dict(
    type='TrackNetV5',
    backbone=dict(
        type='TrackNetV2Backbone', # OK
        in_channels=13
    ),
    neck=dict(
        type='TrackNetV2Neck'# OK
    ),
    head=dict( 
        type='R_STRHead',
        in_channels=64,
        out_channels=3 # <-- ä½ æåˆ°è¿™ç°åœ¨æ˜¯ 3
    )
)


# --- âœ¨âœ¨âœ¨ å·²ä¿®æ”¹çš„è¾…åŠ©å‡½æ•° âœ¨âœ¨âœ¨ ---
def _heatmap_to_coords(heatmap: np.ndarray, threshold: int = 127, min_circularity: float = 0.7):
    """
    ä¸€ä¸ªé²æ£’çš„åæ ‡æå–å‡½æ•°ã€‚
    å®ƒå¯¹çƒ­åŠ›å›¾è¿›è¡ŒäºŒå€¼åŒ–ï¼Œç„¶åå¯»æ‰¾æœ€å¤§ä¸”ç¬¦åˆåœ†åº¦è¦æ±‚çš„è½®å»“çš„è´¨å¿ƒã€‚
    """
    if heatmap.dtype != np.uint8:
        heatmap = heatmap.astype(np.uint8)

    _, binary_map = cv2.threshold(heatmap, threshold, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    valid_contours = []
    if contours:
        for c in contours:
            area = cv2.contourArea(c)
            # é¢„å…ˆè¿‡æ»¤æ‰éå¸¸å°çš„å™ªç‚¹
            if area < 5: 
                continue
            
            perimeter = cv2.arcLength(c, True)
            if perimeter == 0:
                continue
                
            # è®¡ç®—åœ†åº¦
            circularity = 4 * math.pi * (area / (perimeter * perimeter))
            
            if circularity >= min_circularity:
                valid_contours.append(c)

    if valid_contours:
        largest_contour = max(valid_contours, key=cv2.contourArea)
        M = cv2.moments(largest_contour)
        if M["m00"] > 0:
            cx = int(M["m10"] / M["m00"])
            cy = int(M["m01"] / M["m00"])
            return cx, cy

    return None


def draw_comet_tail(frame, points_deque):
    """(æ­¤å‡½æ•°ä¿æŒä¸å˜)"""
    overlay = np.zeros_like(frame, dtype=np.uint8)
    for i in range(1, len(points_deque)):
        if points_deque[i - 1] is None or points_deque[i] is None:
            continue
        alpha = i / len(points_deque)
        line_color = (0, 0, int(alpha * 255))
        pt1 = tuple(points_deque[i - 1])
        pt2 = tuple(points_deque[i])
        cv2.line(overlay, pt1, pt2, line_color, 2)
    frame = cv2.addWeighted(overlay, 1.0, frame, 1.0, 0)
    if points_deque and points_deque[-1] is not None:
        cv2.circle(frame, tuple(points_deque[-1]), 5, (0, 0, 255), -1)
    return frame

# --- 2. â€œæ ¸å¿ƒåŠ å·¥è½¦é—´â€: âœ¨âœ¨âœ¨ å·²é‡æ„çš„ process_video å‡½æ•° âœ¨âœ¨âœ¨ ---
def process_video(video_path: Path, model, device, args, output_root_dir: Path):
    """
    å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆæ‰€æœ‰éœ€è¦çš„è¾“å‡ºæ–‡ä»¶ã€‚
    æ–°é€»è¾‘ï¼šä¸€æ¬¡è¯»å– 3 å¸§ï¼Œæ¨ç† 3 å¸§ï¼Œå†™å…¥ 3 å¸§ï¼Œç„¶åè·³ 3 å¸§ã€‚
    """
    print(f"\nğŸ­ Processing video: {video_path.name}")
    
    video_output_dir = output_root_dir / video_path.stem
    video_output_dir.mkdir(parents=True, exist_ok=True)
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    
    input_size = (288, 512)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    
    trajectory_video_path = video_output_dir / f"{video_path.stem}_trajectory.mp4"
    comparison_video_path = video_output_dir / f"{video_path.stem}_comparison.mp4"
    csv_path = video_output_dir / f"{video_path.stem}_data.csv"
    
    writer_traj = cv2.VideoWriter(str(trajectory_video_path), fourcc, fps, (input_size[1], input_size[0]))
    writer_comp = cv2.VideoWriter(str(comparison_video_path), fourcc, fps, (input_size[1] * 2, input_size[0]))

    # è½¨è¿¹ç‚¹ä¿ç•™ä¸å˜ï¼Œå®ƒåªå…³å¿ƒæœ€è¿‘çš„ `fps` ä¸ªç‚¹
    trajectory_points = deque(maxlen=fps) 
    
    csv_data = []
    detected_frames_count = 0
    
    # é¢„å¤„ç†è½¬æ¢ï¼ˆä¿æŒä¸å˜ï¼‰
    resizer = Resize(keys=['path_prev', 'path', 'path_next'], size=input_size)
    concatenator = ConcatChannels(
        keys=['path_prev', 'path', 'path_next'],
        output_key='image'
    )
    
    # --- æ–°çš„å¾ªç¯é€»è¾‘ ---
    frame_idx_counter = 0
    pbar = tqdm(total=total_frames, desc=f"Processing {video_path.stem}")

    while cap.isOpened():
        # 1. ä¸€æ¬¡æ€§è¯»å– 3 å¸§
        ret1, frame1 = cap.read()
        ret2, frame2 = cap.read()
        ret3, frame3 = cap.read()

        # å¦‚æœä»»ä½•ä¸€å¸§è¯»å–å¤±è´¥ï¼ˆè§†é¢‘æœ«å°¾ï¼‰ï¼Œåˆ™ç»ˆæ­¢å¾ªç¯
        if not ret1 or not ret2 or not ret3:
            break

        # 2. å‡†å¤‡æ¨¡å‹è¾“å…¥
        # (ä½ æåˆ°æ¨¡å‹å†…éƒ¨å¤„ç†ï¼Œæˆ‘ä»¬åªéœ€æŒ‰è½¬æ¢å™¨è¦æ±‚æä¾›3å¸§)
        frame1_rgb = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)
        frame2_rgb = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)
        frame3_rgb = cv2.cvtColor(frame3, cv2.COLOR_BGR2RGB)
        
        data_dict = {'path_prev': frame1_rgb, 'path': frame2_rgb, 'path_next': frame3_rgb}
        data_dict = resizer(data_dict)
        data_dict = concatenator(data_dict)
        
        # å­˜å‚¨è°ƒæ•´å¤§å°åçš„å¸§ï¼Œç”¨äºåç»­ç»˜å›¾
        # data_dict['path_prev'] ç°åœ¨æ˜¯è°ƒæ•´åçš„ frame1
        resized_frames = [data_dict['path_prev'], data_dict['path'], data_dict['path_next']]
        
        image_np = data_dict['image']
        image_tensor = torch.from_numpy(image_np.transpose(2, 0, 1)).float().div(255).unsqueeze(0).to(device)

        # 3. æ‰¹é‡æ¨ç†
        with torch.no_grad():
            # heatmap_preds çš„å½¢çŠ¶æ˜¯ [1, 3, H, W]
            heatmap_preds = model(image_tensor)
        
        # ç§»é™¤ batch ç»´åº¦ï¼Œå¾—åˆ° (3, H, W) çš„ NumPy æ•°ç»„
        heatmaps_np = heatmap_preds.squeeze(0).cpu().numpy()
        threshold_uint8 = int(args.threshold * 255)

        # 4. å¾ªç¯å¤„ç†è¿™ 3 å¸§çš„ç»“æœ
        for i in range(3):
            current_frame_idx = frame_idx_counter + i
            single_heatmap_np = heatmaps_np[i] # å½¢çŠ¶ (H, W)
            heatmap_uint8 = (single_heatmap_np * 255).astype(np.uint8)

            # (A) æå–åæ ‡
            coords = _heatmap_to_coords(heatmap_uint8, threshold=threshold_uint8, min_circularity=args.min_circularity)
            
            # (B) è®°å½• CSV å’Œè½¨è¿¹
            if coords is not None:
                detected_frames_count += 1
                trajectory_points.append(coords)
                csv_row = {'frame_number': current_frame_idx, 'detected': 1, 'x': coords[0], 'y': coords[1]}
            else:
                trajectory_points.append(None)
                csv_row = {'frame_number': current_frame_idx, 'detected': 0, 'x': 0.0, 'y': 0.0}
            csv_data.append(csv_row)
            
            # (C) ç»˜åˆ¶å’Œå†™å…¥è§†é¢‘
            frame_to_draw = cv2.cvtColor(resized_frames[i], cv2.COLOR_RGB2BGR)
            
            # ç»˜åˆ¶è½¨è¿¹è§†é¢‘
            final_traj_frame = draw_comet_tail(frame_to_draw, trajectory_points)
            writer_traj.write(final_traj_frame)

            # ç»˜åˆ¶å¯¹æ¯”è§†é¢‘
            heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
            combined_frame = np.hstack((final_traj_frame, heatmap_color))
            writer_comp.write(combined_frame)

        # 5. æ›´æ–°è®¡æ•°å™¨å’Œè¿›åº¦æ¡ (å…³é”®ï¼)
        frame_idx_counter += 3
        pbar.update(3)
    
    # --- å¾ªç¯ç»“æŸåçš„æ¸…ç† ---
    pbar.close() # å…³é—­è¿›åº¦æ¡

    detection_ratio = (detected_frames_count / total_frames) if total_frames > 0 else 0
    with open(csv_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['frame_number', 'detected', 'x', 'y'])
        writer.writeheader()
        writer.writerows(csv_data)
        f.write("\n")
        f.write(f"total_detected_frame,{detected_frames_count}\n")
        f.write(f"detection_ratio,{detection_ratio:.4f}\n")

    cap.release()
    writer_traj.release()
    writer_comp.release()
    print(f"âœ… Finished processing. Results saved in: {video_output_dir}")

# --- 3. â€œæ€»è°ƒåº¦å®¤â€: main å‡½æ•° (ä¿æŒä¸å˜) ---
def main():
    parser = argparse.ArgumentParser(description="TrackNetV5 Batch Inference Pipeline")
    parser.add_argument('input_dir', type=str, help='Path to the directory containing input videos.')
    parser.add_argument('weights_path', type=str, help='Path to the model weights (.pth file).')
    parser.add_argument('--device', type=str, default='cuda:0', help='Device to use for inference (e.g., "cuda:0" or "cpu").')
    parser.add_argument('--threshold', type=float, default=0.5, help='Confidence threshold for detection (0-1).')
    parser.add_argument('--min-circularity', type=float, default=0.7, help='Minimum circularity for a valid detection (0-1).')
    args = parser.parse_args()

    print("ğŸš€ Starting Batch Inference Pipeline...")
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    
    model = build_model(model_cfg)
    model.load_state_dict(torch.load(args.weights_path, map_location='cpu'))
    model.to(device).eval()
    print(f"âœ… Model loaded from {args.weights_path} and sent to {device}.")

    input_dir = Path(args.input_dir)
    output_root_dir = input_dir / "utracknet_mvat_wbce"
    output_root_dir.mkdir(exist_ok=True)
    
    print("ğŸ” Searching for .mp4 and .mov files...")
    video_files = []
    supported_formats = ['*.mp4', '*.mov', '*.MOV', '*.MP4']
    for fmt in supported_formats:
        video_files.extend(input_dir.glob(fmt))
    
    if not video_files:
        print(f"âŒ No supported video files (.mp4, .mov) found in {input_dir}. Exiting.")
        return
        
    video_files = sorted(list(set(video_files)))
    print(f"Found {len(video_files)} videos to process.")
    
    for video_path in video_files:
        process_video(video_path, model, device, args, output_root_dir)

    print(f"\nğŸ‰ğŸ‰ğŸ‰ All videos processed! Check the results in: {output_root_dir} ğŸ‰ğŸ‰ğŸ‰")

if __name__ == '__main__':
    main()